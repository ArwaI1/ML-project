{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "yyF69R6F5aIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0nEVUaAxT4Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#Linear Regression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#Support Vector Machine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#Neural Network\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD, Adam\n",
        "#Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "7SImCp9R5hrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VageWCfgx198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77eee491-be5f-43c6-c5a4-6999e8ea9632"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            alx     aly      alz       glx      gly      glz     arx     ary  \\\n",
              "0        2.1849 -9.6967  0.63077  0.103900 -0.84053 -0.68762 -8.6499 -4.5781   \n",
              "1        2.3876 -9.5080  0.68389  0.085343 -0.83865 -0.68369 -8.6275 -4.3198   \n",
              "2        2.4086 -9.5674  0.68113  0.085343 -0.83865 -0.68369 -8.5055 -4.2772   \n",
              "3        2.1814 -9.4301  0.55031  0.085343 -0.83865 -0.68369 -8.6279 -4.3163   \n",
              "4        2.4173 -9.3889  0.71098  0.085343 -0.83865 -0.68369 -8.7008 -4.1459   \n",
              "...         ...     ...      ...       ...      ...      ...     ...     ...   \n",
              "1215740  1.7849 -9.8287  0.29725 -0.341370 -0.90056 -0.61493 -3.7198 -8.9071   \n",
              "1215741  1.8687 -9.8766  0.46236 -0.341370 -0.90056 -0.61493 -3.7160 -8.7455   \n",
              "1215742  1.6928 -9.9290  0.16631 -0.341370 -0.90056 -0.61493 -3.8824 -9.1155   \n",
              "1215743  1.5279 -9.6306  0.30458 -0.341370 -0.90056 -0.61493 -3.5564 -9.1441   \n",
              "1215744  1.6614 -9.8398  0.18088 -0.332100 -0.90432 -0.61886 -3.9035 -8.9324   \n",
              "\n",
              "              arz       grx      gry       grz  Activity    subject  \n",
              "0        0.187760 -0.449020 -1.01030  0.034483         0   subject1  \n",
              "1        0.023595 -0.449020 -1.01030  0.034483         0   subject1  \n",
              "2        0.275720 -0.449020 -1.01030  0.034483         0   subject1  \n",
              "3        0.367520 -0.456860 -1.00820  0.025862         0   subject1  \n",
              "4        0.407290 -0.456860 -1.00820  0.025862         0   subject1  \n",
              "...           ...       ...      ...       ...       ...        ...  \n",
              "1215740  0.294230  0.041176 -0.99384 -0.480600         0  subject10  \n",
              "1215741  0.448140  0.041176 -0.99384 -0.480600         0  subject10  \n",
              "1215742  0.450480  0.041176 -0.99384 -0.480600         0  subject10  \n",
              "1215743  0.594880  0.041176 -0.99384 -0.480600         0  subject10  \n",
              "1215744  0.761710  0.035294 -1.02050 -0.471980         0  subject10  \n",
              "\n",
              "[1215745 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8733029-53b0-41f8-bc8b-b05df6d214ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alx</th>\n",
              "      <th>aly</th>\n",
              "      <th>alz</th>\n",
              "      <th>glx</th>\n",
              "      <th>gly</th>\n",
              "      <th>glz</th>\n",
              "      <th>arx</th>\n",
              "      <th>ary</th>\n",
              "      <th>arz</th>\n",
              "      <th>grx</th>\n",
              "      <th>gry</th>\n",
              "      <th>grz</th>\n",
              "      <th>Activity</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.1849</td>\n",
              "      <td>-9.6967</td>\n",
              "      <td>0.63077</td>\n",
              "      <td>0.103900</td>\n",
              "      <td>-0.84053</td>\n",
              "      <td>-0.68762</td>\n",
              "      <td>-8.6499</td>\n",
              "      <td>-4.5781</td>\n",
              "      <td>0.187760</td>\n",
              "      <td>-0.449020</td>\n",
              "      <td>-1.01030</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0</td>\n",
              "      <td>subject1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.3876</td>\n",
              "      <td>-9.5080</td>\n",
              "      <td>0.68389</td>\n",
              "      <td>0.085343</td>\n",
              "      <td>-0.83865</td>\n",
              "      <td>-0.68369</td>\n",
              "      <td>-8.6275</td>\n",
              "      <td>-4.3198</td>\n",
              "      <td>0.023595</td>\n",
              "      <td>-0.449020</td>\n",
              "      <td>-1.01030</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0</td>\n",
              "      <td>subject1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.4086</td>\n",
              "      <td>-9.5674</td>\n",
              "      <td>0.68113</td>\n",
              "      <td>0.085343</td>\n",
              "      <td>-0.83865</td>\n",
              "      <td>-0.68369</td>\n",
              "      <td>-8.5055</td>\n",
              "      <td>-4.2772</td>\n",
              "      <td>0.275720</td>\n",
              "      <td>-0.449020</td>\n",
              "      <td>-1.01030</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0</td>\n",
              "      <td>subject1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.1814</td>\n",
              "      <td>-9.4301</td>\n",
              "      <td>0.55031</td>\n",
              "      <td>0.085343</td>\n",
              "      <td>-0.83865</td>\n",
              "      <td>-0.68369</td>\n",
              "      <td>-8.6279</td>\n",
              "      <td>-4.3163</td>\n",
              "      <td>0.367520</td>\n",
              "      <td>-0.456860</td>\n",
              "      <td>-1.00820</td>\n",
              "      <td>0.025862</td>\n",
              "      <td>0</td>\n",
              "      <td>subject1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.4173</td>\n",
              "      <td>-9.3889</td>\n",
              "      <td>0.71098</td>\n",
              "      <td>0.085343</td>\n",
              "      <td>-0.83865</td>\n",
              "      <td>-0.68369</td>\n",
              "      <td>-8.7008</td>\n",
              "      <td>-4.1459</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>-0.456860</td>\n",
              "      <td>-1.00820</td>\n",
              "      <td>0.025862</td>\n",
              "      <td>0</td>\n",
              "      <td>subject1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215740</th>\n",
              "      <td>1.7849</td>\n",
              "      <td>-9.8287</td>\n",
              "      <td>0.29725</td>\n",
              "      <td>-0.341370</td>\n",
              "      <td>-0.90056</td>\n",
              "      <td>-0.61493</td>\n",
              "      <td>-3.7198</td>\n",
              "      <td>-8.9071</td>\n",
              "      <td>0.294230</td>\n",
              "      <td>0.041176</td>\n",
              "      <td>-0.99384</td>\n",
              "      <td>-0.480600</td>\n",
              "      <td>0</td>\n",
              "      <td>subject10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215741</th>\n",
              "      <td>1.8687</td>\n",
              "      <td>-9.8766</td>\n",
              "      <td>0.46236</td>\n",
              "      <td>-0.341370</td>\n",
              "      <td>-0.90056</td>\n",
              "      <td>-0.61493</td>\n",
              "      <td>-3.7160</td>\n",
              "      <td>-8.7455</td>\n",
              "      <td>0.448140</td>\n",
              "      <td>0.041176</td>\n",
              "      <td>-0.99384</td>\n",
              "      <td>-0.480600</td>\n",
              "      <td>0</td>\n",
              "      <td>subject10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215742</th>\n",
              "      <td>1.6928</td>\n",
              "      <td>-9.9290</td>\n",
              "      <td>0.16631</td>\n",
              "      <td>-0.341370</td>\n",
              "      <td>-0.90056</td>\n",
              "      <td>-0.61493</td>\n",
              "      <td>-3.8824</td>\n",
              "      <td>-9.1155</td>\n",
              "      <td>0.450480</td>\n",
              "      <td>0.041176</td>\n",
              "      <td>-0.99384</td>\n",
              "      <td>-0.480600</td>\n",
              "      <td>0</td>\n",
              "      <td>subject10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215743</th>\n",
              "      <td>1.5279</td>\n",
              "      <td>-9.6306</td>\n",
              "      <td>0.30458</td>\n",
              "      <td>-0.341370</td>\n",
              "      <td>-0.90056</td>\n",
              "      <td>-0.61493</td>\n",
              "      <td>-3.5564</td>\n",
              "      <td>-9.1441</td>\n",
              "      <td>0.594880</td>\n",
              "      <td>0.041176</td>\n",
              "      <td>-0.99384</td>\n",
              "      <td>-0.480600</td>\n",
              "      <td>0</td>\n",
              "      <td>subject10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215744</th>\n",
              "      <td>1.6614</td>\n",
              "      <td>-9.8398</td>\n",
              "      <td>0.18088</td>\n",
              "      <td>-0.332100</td>\n",
              "      <td>-0.90432</td>\n",
              "      <td>-0.61886</td>\n",
              "      <td>-3.9035</td>\n",
              "      <td>-8.9324</td>\n",
              "      <td>0.761710</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>-1.02050</td>\n",
              "      <td>-0.471980</td>\n",
              "      <td>0</td>\n",
              "      <td>subject10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1215745 rows Ã— 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8733029-53b0-41f8-bc8b-b05df6d214ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8733029-53b0-41f8-bc8b-b05df6d214ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8733029-53b0-41f8-bc8b-b05df6d214ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-62d5912e-3874-453e-b8fa-af517513c6ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-62d5912e-3874-453e-b8fa-af517513c6ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-62d5912e-3874-453e-b8fa-af517513c6ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dff"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dff = pd.read_csv('/content/mhealth_raw_data.csv')\n",
        "display(dff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vqTipRTyJxt"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ricVQy3Ox5KY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769ba34b-34cd-437f-b246-2a01e0dfeef8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dff.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaGB-CENyRXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd616d19-1f29-4ba8-d604-7851496389d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alx         0\n",
              "aly         0\n",
              "alz         0\n",
              "glx         0\n",
              "gly         0\n",
              "glz         0\n",
              "arx         0\n",
              "ary         0\n",
              "arz         0\n",
              "grx         0\n",
              "gry         0\n",
              "grz         0\n",
              "Activity    0\n",
              "subject     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dff.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6mLMQI4yVe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2d11f0-0cd5-4c5f-d067-cb8da4da6312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alx         0\n",
              "aly         0\n",
              "alz         0\n",
              "glx         0\n",
              "gly         0\n",
              "glz         0\n",
              "arx         0\n",
              "ary         0\n",
              "arz         0\n",
              "grx         0\n",
              "gry         0\n",
              "grz         0\n",
              "Activity    0\n",
              "subject     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Fill null values in numeric columns with mode\n",
        "numeric_cols = dff.select_dtypes(include=[np.number]).columns\n",
        "dff[numeric_cols] = dff[numeric_cols].apply(lambda col: col.fillna(col.mode()[0]))\n",
        "\n",
        "# Check for any remaining null values\n",
        "dff.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti5vgdoWymQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e85039-3db6-482f-c617-53769c1419bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1215745 entries, 0 to 1215744\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count    Dtype  \n",
            "---  ------    --------------    -----  \n",
            " 0   alx       1215745 non-null  float64\n",
            " 1   aly       1215745 non-null  float64\n",
            " 2   alz       1215745 non-null  float64\n",
            " 3   glx       1215745 non-null  float64\n",
            " 4   gly       1215745 non-null  float64\n",
            " 5   glz       1215745 non-null  float64\n",
            " 6   arx       1215745 non-null  float64\n",
            " 7   ary       1215745 non-null  float64\n",
            " 8   arz       1215745 non-null  float64\n",
            " 9   grx       1215745 non-null  float64\n",
            " 10  gry       1215745 non-null  float64\n",
            " 11  grz       1215745 non-null  float64\n",
            " 12  Activity  1215745 non-null  int64  \n",
            " 13  subject   1215745 non-null  object \n",
            "dtypes: float64(12), int64(1), object(1)\n",
            "memory usage: 129.9+ MB\n"
          ]
        }
      ],
      "source": [
        "dff.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dff.drop(columns=['subject'], inplace=True)"
      ],
      "metadata": {
        "id": "8qL2Xrt2wUav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5vU1NSryqdd"
      },
      "outputs": [],
      "source": [
        "# dff.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3VaaEDtywGC"
      },
      "source": [
        "## Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dff['Activity'].value_counts()"
      ],
      "metadata": {
        "id": "SEIG7Ra3otju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k72_OPNry0Z7"
      },
      "outputs": [],
      "source": [
        "# # Take a random sample of size 10000\n",
        "# sample_size = 10000\n",
        "# df = dff.sample(n=sample_size, random_state=42)  # Use a specific random state for reproducibility\n",
        "\n",
        "# X = df[['alx', 'aly', 'alz', 'glx', 'gly', 'glz', 'arx', 'ary', 'arz', 'grx', 'gry', 'grz']]\n",
        "# y = df['Activity']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = dff['Activity'].value_counts()\n",
        "\n",
        "#  minimum count among all classes\n",
        "min_count = class_counts.min()\n",
        "\n",
        "# Sample each class with a count equal to min_count\n",
        "sampled_dfs = []\n",
        "for activity, count in class_counts.items():\n",
        "    sampled_df = dff[dff['Activity'] == activity].sample(min_count, random_state=42)\n",
        "    sampled_dfs.append(sampled_df)\n",
        "\n",
        "# Concatenate the sampled DataFrames to get a balanced sample\n",
        "balanced_sample = pd.concat(sampled_dfs)\n",
        "\n",
        "\n",
        "X = balanced_sample[['alx', 'aly', 'alz', 'glx', 'gly', 'glz', 'arx', 'ary', 'arz', 'grx', 'gry', 'grz']]\n",
        "y = balanced_sample['Activity']\n",
        "\n"
      ],
      "metadata": {
        "id": "sjYeiD0i0l79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling:\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "cvJbNUcgTwEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tfOhs1WFTwrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLU3bM3DykKP"
      },
      "source": [
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yau1xjnqMk3"
      },
      "outputs": [],
      "source": [
        "# empty dictionary\n",
        "metrics_dict = {\n",
        "    'accuracy': {},\n",
        "    'precision': {},\n",
        "    'recall': {},\n",
        "    'f1_score': {},\n",
        "    'confusion_matrix':{},\n",
        "    'MSE' : {}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_slJ1O31dCN"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Tc0dZlIQj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7df9190-b1fb-45c5-d8ce-2452197f5131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9403495723317218\n",
            "Precision: 0.9411102272719324\n",
            "Recall: 0.9403495723317218\n",
            "F1-score: 0.9360633304655549\n",
            "Confusion Matrix:\n",
            "[[1139   57   44   23  173   97   91  121  109  101   42   24   57]\n",
            " [   0 2035    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0 2090    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [   4    0    0    0 2079    0    2    0    2    0    0    0    1]\n",
            " [  23    1    1    0   61 1911   11    2   32    1    2    0    3]\n",
            " [   2    3    0    0    0    0 1965   12    4    0    0    0    0]\n",
            " [   3    0    0    0    0    0    7 2056    5    1    0    0    0]\n",
            " [   3    0    0    0    2    0    8    9 2070    0    0    0    0]\n",
            " [   2    0    0    0    0    0    0    0    3 2022    0    0    1]\n",
            " [   3    0    0    0    6    1    0    3    1    0 1994   74   15]\n",
            " [   2    0    0    0   14    0    0    0    0    1   95 1988   20]\n",
            " [  30    0    0    0   24    7    3    4    6    1   91   53 1832]]\n",
            "\n",
            "\n",
            "Example predictions for KNN:\n",
            "Sample 1: Predicted=6, Actual=6\n",
            "Sample 2: Predicted=1, Actual=1\n",
            "Sample 3: Predicted=8, Actual=8\n",
            "Sample 4: Predicted=1, Actual=1\n",
            "Sample 5: Predicted=5, Actual=5\n"
          ]
        }
      ],
      "source": [
        "# Create and train the KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors=7)  # Set the number of neighbors to 7\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Store the accuracy\n",
        "metrics_dict['accuracy']['KNN'] = accuracy\n",
        "metrics_dict['precision']['KNN'] = precision\n",
        "metrics_dict['recall']['KNN'] = recall\n",
        "metrics_dict['f1_score']['KNN'] = f1\n",
        "metrics_dict['confusion_matrix']['KNN'] = confusion\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion}\\n\")\n",
        "\n",
        "# Interpretation of model predictions for KNN\n",
        "print(\"\\nExample predictions for KNN:\")\n",
        "for i in range(min(5, len(y_test))):  # Print 5 samples or less\n",
        "    pred_value = y_pred[i]\n",
        "    actual_value = y_test.iloc[i]\n",
        "    print(f\"Sample {i+1}: Predicted={pred_value}, Actual={actual_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGI8mADo2JkL"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRA7J4J_1hr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8249ca-8c16-43b7-e69f-3cf47bde5988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.68860940592738\n",
            "\n",
            "Example predictions for Lineaar Regression:\n",
            "Sample 1: Predicted=6, Actual=6\n",
            "Sample 2: Predicted=1, Actual=1\n",
            "Sample 3: Predicted=8, Actual=8\n",
            "Sample 4: Predicted=1, Actual=1\n",
            "Sample 5: Predicted=5, Actual=5\n"
          ]
        }
      ],
      "source": [
        "linear_regression = LinearRegression()\n",
        "linear_regression.fit(X_train, y_train)\n",
        "\n",
        "# predictions and Mean Squared Error\n",
        "linear_regression_predictions = linear_regression.predict(X_test)\n",
        "mse = mean_squared_error(y_test, linear_regression_predictions)\n",
        "\n",
        "# Store the mse\n",
        "metrics_dict['accuracy']['Linear Regression'] = None\n",
        "metrics_dict['MSE']['Linear Regression'] = mse\n",
        "\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Interpretation of model predictions for Lineaar Regression\n",
        "print(\"\\nExample predictions for Lineaar Regression:\")\n",
        "for i in range(min(5, len(y_test))):  # Print 5 samples or less\n",
        "    pred_value = y_pred[i]\n",
        "    actual_value = y_test.iloc[i]\n",
        "    print(f\"Sample {i+1}: Predicted={pred_value}, Actual={actual_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3czuiZA32xEG"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the parameter grid for grid search\n",
        "# param_grid = {\n",
        "#         'C':[0.1,1,10,100],\n",
        "#         'kernel': ['linear', 'rbf', 'poly']\n",
        "# }\n",
        "\n",
        "# #SVM\n",
        "# svm = SVC()\n",
        "\n",
        "# # Perform grid search with cross-validation\n",
        "# grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "# print(\"Best Accuracy: \", grid_search.best_score_)\n",
        "\n",
        "# # Evaluation\n",
        "# best_model = grid_search.best_estimator_\n",
        "# test_accuracy = best_model.score(X_test, y_test)\n",
        "# print(\"Test Accuracy: \", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "fQVJBqmQg83U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf4jG6ox9htZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2869bd80-38d8-4ddc-a7cd-a08655f3219c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9534399404983265\n",
            "Precision: 0.9515233359234337\n",
            "Recall: 0.9534399404983265\n",
            "F1-score: 0.9514069801651617\n",
            "Confusion Matrix:\n",
            "[[1367   62   42   26  109   79   73   61   93   65   24   19   58]\n",
            " [   1 2034    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0 2090    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [  25    0    0    0 2056    3    1    0    3    0    0    0    0]\n",
            " [  86    0    0    0   25 1928    3    0    4    0    0    0    2]\n",
            " [   8    4    0    0    0    0 1963   11    0    0    0    0    0]\n",
            " [   3    0    0    0    0    0   17 2046    5    1    0    0    0]\n",
            " [  12    0    0    0    2    0    5    8 2063    1    0    0    1]\n",
            " [   7    0    0    0    0    0    0    0    0 2021    0    0    0]\n",
            " [  29    0    0    0    0    0    0    0    0    0 1997   58   13]\n",
            " [  26    0    0    0    1    0    0    0    0    0   53 2020   20]\n",
            " [  51    0    0    0    3    2    0    0    2    0   23   22 1948]]\n",
            "\n",
            "\n",
            "Example predictions for SVM:\n",
            "Sample 1: Predicted=6, Actual=6\n",
            "Sample 2: Predicted=1, Actual=1\n",
            "Sample 3: Predicted=8, Actual=8\n",
            "Sample 4: Predicted=1, Actual=1\n",
            "Sample 5: Predicted=5, Actual=5\n"
          ]
        }
      ],
      "source": [
        "# Create and train the SVM model\n",
        "svm = SVC(kernel='rbf', C=100)  # Use the RBF kernel\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Store the accuracy\n",
        "metrics_dict['accuracy']['SVM'] = accuracy\n",
        "metrics_dict['precision']['SVM'] = precision\n",
        "metrics_dict['recall']['SVM'] = recall\n",
        "metrics_dict['f1_score']['SVM'] = f1\n",
        "metrics_dict['confusion_matrix']['SVM'] = confusion\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion}\\n\")\n",
        "\n",
        "# Interpretation of model predictions for SVM\n",
        "print(\"\\nExample predictions for SVM:\")\n",
        "for i in range(min(5, len(y_test))):   # Print 5 samples or less\n",
        "    pred_value = y_pred[i]\n",
        "    actual_value = y_test.iloc[i]\n",
        "    print(f\"Sample {i+1}: Predicted={pred_value}, Actual={actual_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq07oe4B2bem"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM9vk8LiIi-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219239eb-bf89-4654-9b90-e9ef70e50a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 1.3176 - accuracy: 0.5923\n",
            "Epoch 2/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.7627 - accuracy: 0.7612\n",
            "Epoch 3/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.6126 - accuracy: 0.8083\n",
            "Epoch 4/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.5264 - accuracy: 0.8361\n",
            "Epoch 5/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.4690 - accuracy: 0.8549\n",
            "Epoch 6/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.4276 - accuracy: 0.8684\n",
            "Epoch 7/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.3963 - accuracy: 0.8785\n",
            "Epoch 8/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.3719 - accuracy: 0.8872\n",
            "Epoch 9/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.3523 - accuracy: 0.8936\n",
            "Epoch 10/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.3356 - accuracy: 0.8982\n",
            "Epoch 11/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.3215 - accuracy: 0.9029\n",
            "Epoch 12/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.3100 - accuracy: 0.9063\n",
            "Epoch 13/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2990 - accuracy: 0.9087\n",
            "Epoch 14/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2900 - accuracy: 0.9125\n",
            "Epoch 15/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.2819 - accuracy: 0.9143\n",
            "Epoch 16/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2747 - accuracy: 0.9166\n",
            "Epoch 17/200\n",
            "3362/3362 [==============================] - 13s 4ms/step - loss: 0.2684 - accuracy: 0.9187\n",
            "Epoch 18/200\n",
            "3362/3362 [==============================] - 12s 4ms/step - loss: 0.2624 - accuracy: 0.9210\n",
            "Epoch 19/200\n",
            "3362/3362 [==============================] - 11s 3ms/step - loss: 0.2572 - accuracy: 0.9222\n",
            "Epoch 20/200\n",
            "3362/3362 [==============================] - 11s 3ms/step - loss: 0.2522 - accuracy: 0.9237\n",
            "Epoch 21/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2479 - accuracy: 0.9253\n",
            "Epoch 22/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2438 - accuracy: 0.9267\n",
            "Epoch 23/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2395 - accuracy: 0.9279\n",
            "Epoch 24/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.2360 - accuracy: 0.9291\n",
            "Epoch 25/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.2321 - accuracy: 0.9296\n",
            "Epoch 26/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2292 - accuracy: 0.9315\n",
            "Epoch 27/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2260 - accuracy: 0.9321\n",
            "Epoch 28/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2233 - accuracy: 0.9332\n",
            "Epoch 29/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2201 - accuracy: 0.9341\n",
            "Epoch 30/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.2181 - accuracy: 0.9349\n",
            "Epoch 31/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2150 - accuracy: 0.9361\n",
            "Epoch 32/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2127 - accuracy: 0.9364\n",
            "Epoch 33/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.2102 - accuracy: 0.9373\n",
            "Epoch 34/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2081 - accuracy: 0.9379\n",
            "Epoch 35/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2061 - accuracy: 0.9385\n",
            "Epoch 36/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.2040 - accuracy: 0.9395\n",
            "Epoch 37/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.2019 - accuracy: 0.9399\n",
            "Epoch 38/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.2002 - accuracy: 0.9405\n",
            "Epoch 39/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1985 - accuracy: 0.9406\n",
            "Epoch 40/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1966 - accuracy: 0.9407\n",
            "Epoch 41/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1950 - accuracy: 0.9425\n",
            "Epoch 42/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1933 - accuracy: 0.9431\n",
            "Epoch 43/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1919 - accuracy: 0.9428\n",
            "Epoch 44/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1898 - accuracy: 0.9438\n",
            "Epoch 45/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1891 - accuracy: 0.9438\n",
            "Epoch 46/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1877 - accuracy: 0.9440\n",
            "Epoch 47/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1859 - accuracy: 0.9450\n",
            "Epoch 48/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1847 - accuracy: 0.9452\n",
            "Epoch 49/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1834 - accuracy: 0.9459\n",
            "Epoch 50/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1820 - accuracy: 0.9463\n",
            "Epoch 51/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1813 - accuracy: 0.9463\n",
            "Epoch 52/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1794 - accuracy: 0.9469\n",
            "Epoch 53/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1789 - accuracy: 0.9475\n",
            "Epoch 54/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1773 - accuracy: 0.9471\n",
            "Epoch 55/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1763 - accuracy: 0.9475\n",
            "Epoch 56/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1754 - accuracy: 0.9482\n",
            "Epoch 57/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1747 - accuracy: 0.9482\n",
            "Epoch 58/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1736 - accuracy: 0.9489\n",
            "Epoch 59/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1727 - accuracy: 0.9487\n",
            "Epoch 60/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1717 - accuracy: 0.9496\n",
            "Epoch 61/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1705 - accuracy: 0.9496\n",
            "Epoch 62/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1694 - accuracy: 0.9500\n",
            "Epoch 63/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1690 - accuracy: 0.9504\n",
            "Epoch 64/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1680 - accuracy: 0.9504\n",
            "Epoch 65/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1676 - accuracy: 0.9505\n",
            "Epoch 66/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1665 - accuracy: 0.9505\n",
            "Epoch 67/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1664 - accuracy: 0.9504\n",
            "Epoch 68/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1654 - accuracy: 0.9514\n",
            "Epoch 69/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1643 - accuracy: 0.9515\n",
            "Epoch 70/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1636 - accuracy: 0.9518\n",
            "Epoch 71/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1633 - accuracy: 0.9518\n",
            "Epoch 72/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1617 - accuracy: 0.9521\n",
            "Epoch 73/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1618 - accuracy: 0.9523\n",
            "Epoch 74/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1607 - accuracy: 0.9528\n",
            "Epoch 75/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1599 - accuracy: 0.9532\n",
            "Epoch 76/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1591 - accuracy: 0.9530\n",
            "Epoch 77/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1587 - accuracy: 0.9530\n",
            "Epoch 78/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1586 - accuracy: 0.9533\n",
            "Epoch 79/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1577 - accuracy: 0.9530\n",
            "Epoch 80/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1573 - accuracy: 0.9537\n",
            "Epoch 81/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1572 - accuracy: 0.9538\n",
            "Epoch 82/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1558 - accuracy: 0.9539\n",
            "Epoch 83/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1556 - accuracy: 0.9537\n",
            "Epoch 84/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1546 - accuracy: 0.9546\n",
            "Epoch 85/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1538 - accuracy: 0.9545\n",
            "Epoch 86/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1537 - accuracy: 0.9548\n",
            "Epoch 87/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1530 - accuracy: 0.9544\n",
            "Epoch 88/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1526 - accuracy: 0.9546\n",
            "Epoch 89/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1520 - accuracy: 0.9548\n",
            "Epoch 90/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1514 - accuracy: 0.9555\n",
            "Epoch 91/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1512 - accuracy: 0.9559\n",
            "Epoch 92/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1504 - accuracy: 0.9559\n",
            "Epoch 93/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1494 - accuracy: 0.9557\n",
            "Epoch 94/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1496 - accuracy: 0.9563\n",
            "Epoch 95/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1496 - accuracy: 0.9559\n",
            "Epoch 96/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1485 - accuracy: 0.9558\n",
            "Epoch 97/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1480 - accuracy: 0.9567\n",
            "Epoch 98/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1476 - accuracy: 0.9564\n",
            "Epoch 99/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1470 - accuracy: 0.9564\n",
            "Epoch 100/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1471 - accuracy: 0.9569\n",
            "Epoch 101/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1465 - accuracy: 0.9566\n",
            "Epoch 102/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1462 - accuracy: 0.9572\n",
            "Epoch 103/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1455 - accuracy: 0.9569\n",
            "Epoch 104/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1451 - accuracy: 0.9575\n",
            "Epoch 105/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1447 - accuracy: 0.9577\n",
            "Epoch 106/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1440 - accuracy: 0.9574\n",
            "Epoch 107/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1438 - accuracy: 0.9572\n",
            "Epoch 108/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1428 - accuracy: 0.9579\n",
            "Epoch 109/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1432 - accuracy: 0.9573\n",
            "Epoch 110/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1422 - accuracy: 0.9574\n",
            "Epoch 111/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1422 - accuracy: 0.9581\n",
            "Epoch 112/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1420 - accuracy: 0.9581\n",
            "Epoch 113/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1413 - accuracy: 0.9581\n",
            "Epoch 114/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1406 - accuracy: 0.9587\n",
            "Epoch 115/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1405 - accuracy: 0.9585\n",
            "Epoch 116/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1405 - accuracy: 0.9587\n",
            "Epoch 117/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1396 - accuracy: 0.9588\n",
            "Epoch 118/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1394 - accuracy: 0.9587\n",
            "Epoch 119/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1383 - accuracy: 0.9595\n",
            "Epoch 120/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1389 - accuracy: 0.9593\n",
            "Epoch 121/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1385 - accuracy: 0.9591\n",
            "Epoch 122/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1379 - accuracy: 0.9595\n",
            "Epoch 123/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1373 - accuracy: 0.9598\n",
            "Epoch 124/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1377 - accuracy: 0.9596\n",
            "Epoch 125/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1377 - accuracy: 0.9588\n",
            "Epoch 126/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1368 - accuracy: 0.9597\n",
            "Epoch 127/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1368 - accuracy: 0.9595\n",
            "Epoch 128/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1362 - accuracy: 0.9605\n",
            "Epoch 129/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1357 - accuracy: 0.9597\n",
            "Epoch 130/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1352 - accuracy: 0.9603\n",
            "Epoch 131/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1356 - accuracy: 0.9602\n",
            "Epoch 132/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1345 - accuracy: 0.9603\n",
            "Epoch 133/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1345 - accuracy: 0.9608\n",
            "Epoch 134/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1342 - accuracy: 0.9607\n",
            "Epoch 135/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1346 - accuracy: 0.9602\n",
            "Epoch 136/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1336 - accuracy: 0.9607\n",
            "Epoch 137/200\n",
            "3362/3362 [==============================] - 8s 2ms/step - loss: 0.1332 - accuracy: 0.9607\n",
            "Epoch 138/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1324 - accuracy: 0.9612\n",
            "Epoch 139/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1329 - accuracy: 0.9609\n",
            "Epoch 140/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1323 - accuracy: 0.9611\n",
            "Epoch 141/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1330 - accuracy: 0.9611\n",
            "Epoch 142/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1318 - accuracy: 0.9616\n",
            "Epoch 143/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1316 - accuracy: 0.9609\n",
            "Epoch 144/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1306 - accuracy: 0.9618\n",
            "Epoch 145/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1311 - accuracy: 0.9615\n",
            "Epoch 146/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1311 - accuracy: 0.9611\n",
            "Epoch 147/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1306 - accuracy: 0.9617\n",
            "Epoch 148/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1304 - accuracy: 0.9619\n",
            "Epoch 149/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1300 - accuracy: 0.9616\n",
            "Epoch 150/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1299 - accuracy: 0.9619\n",
            "Epoch 151/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1291 - accuracy: 0.9623\n",
            "Epoch 152/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1294 - accuracy: 0.9619\n",
            "Epoch 153/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1293 - accuracy: 0.9619\n",
            "Epoch 154/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1294 - accuracy: 0.9621\n",
            "Epoch 155/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1283 - accuracy: 0.9627\n",
            "Epoch 156/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1288 - accuracy: 0.9622\n",
            "Epoch 157/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1281 - accuracy: 0.9624\n",
            "Epoch 158/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1285 - accuracy: 0.9622\n",
            "Epoch 159/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1279 - accuracy: 0.9628\n",
            "Epoch 160/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1278 - accuracy: 0.9628\n",
            "Epoch 161/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1274 - accuracy: 0.9632\n",
            "Epoch 162/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1264 - accuracy: 0.9631\n",
            "Epoch 163/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1272 - accuracy: 0.9626\n",
            "Epoch 164/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1264 - accuracy: 0.9633\n",
            "Epoch 165/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1261 - accuracy: 0.9626\n",
            "Epoch 166/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1260 - accuracy: 0.9630\n",
            "Epoch 167/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1255 - accuracy: 0.9633\n",
            "Epoch 168/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1259 - accuracy: 0.9627\n",
            "Epoch 169/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1252 - accuracy: 0.9634\n",
            "Epoch 170/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1250 - accuracy: 0.9633\n",
            "Epoch 171/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1244 - accuracy: 0.9640\n",
            "Epoch 172/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1248 - accuracy: 0.9636\n",
            "Epoch 173/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1244 - accuracy: 0.9637\n",
            "Epoch 174/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1242 - accuracy: 0.9635\n",
            "Epoch 175/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1240 - accuracy: 0.9636\n",
            "Epoch 176/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1237 - accuracy: 0.9638\n",
            "Epoch 177/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1233 - accuracy: 0.9639\n",
            "Epoch 178/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1236 - accuracy: 0.9640\n",
            "Epoch 179/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1232 - accuracy: 0.9640\n",
            "Epoch 180/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1227 - accuracy: 0.9646\n",
            "Epoch 181/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1230 - accuracy: 0.9641\n",
            "Epoch 182/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1226 - accuracy: 0.9638\n",
            "Epoch 183/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1226 - accuracy: 0.9638\n",
            "Epoch 184/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1226 - accuracy: 0.9642\n",
            "Epoch 185/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1218 - accuracy: 0.9648\n",
            "Epoch 186/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1228 - accuracy: 0.9642\n",
            "Epoch 187/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1213 - accuracy: 0.9648\n",
            "Epoch 188/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1216 - accuracy: 0.9647\n",
            "Epoch 189/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1209 - accuracy: 0.9647\n",
            "Epoch 190/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1215 - accuracy: 0.9644\n",
            "Epoch 191/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1209 - accuracy: 0.9646\n",
            "Epoch 192/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1207 - accuracy: 0.9645\n",
            "Epoch 193/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1205 - accuracy: 0.9645\n",
            "Epoch 194/200\n",
            "3362/3362 [==============================] - 8s 2ms/step - loss: 0.1206 - accuracy: 0.9653\n",
            "Epoch 195/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1197 - accuracy: 0.9650\n",
            "Epoch 196/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1199 - accuracy: 0.9650\n",
            "Epoch 197/200\n",
            "3362/3362 [==============================] - 6s 2ms/step - loss: 0.1196 - accuracy: 0.9652\n",
            "Epoch 198/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1199 - accuracy: 0.9652\n",
            "Epoch 199/200\n",
            "3362/3362 [==============================] - 7s 2ms/step - loss: 0.1190 - accuracy: 0.9654\n",
            "Epoch 200/200\n",
            "3362/3362 [==============================] - 5s 2ms/step - loss: 0.1186 - accuracy: 0.9654\n",
            "841/841 [==============================] - 1s 1ms/step\n",
            "Accuracy (NN): 0.9654412865638733\n",
            "Precision (NN): 0.9499435003035965\n",
            "Recall (NN): 0.9513201933804388\n",
            "F1-score (NN): 0.949350212902209\n",
            "Confusion Matrix (NN):\n",
            "[[1377   49   41   31  117   60   83   62   71   57   35   22   73]\n",
            " [   2 2029    0    0    0    0    4    0    0    0    0    0    0]\n",
            " [   0    0 2090    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [  15    0    0    0 2064    3    2    0    0    0    2    0    2]\n",
            " [ 102    0    0    0   37 1889    4    0   11    0    1    0    4]\n",
            " [   5    2    0    0    0    1 1977    0    1    0    0    0    0]\n",
            " [   9    0    0    0    0    0   29 2028    5    0    0    0    1]\n",
            " [  19    0    0    0    4    1    8    4 2054    0    2    0    0]\n",
            " [  16    0    0    0    0    0    0    1    0 2009    1    0    1]\n",
            " [   7    0    0    0    0    0    0    0    0    0 2037   34   19]\n",
            " [  12    0    0    0    2    2    0    0    0    0  116 1968   20]\n",
            " [  40    0    0    0    3    5    1    0    0    0   31   17 1954]]\n",
            "\n",
            "\n",
            "Example predictions for NN:\n",
            "Sample 1: Predicted=6, Actual=6\n",
            "Sample 2: Predicted=1, Actual=1\n",
            "Sample 3: Predicted=8, Actual=8\n",
            "Sample 4: Predicted=1, Actual=1\n",
            "Sample 5: Predicted=5, Actual=5\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network model\n",
        "NN_model2 = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y)), activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "NN_model2.compile(optimizer='sgd',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "hist = NN_model2.fit(X_train, y_train, batch_size=32, epochs=200)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_nn = NN_model2.predict(X_test)\n",
        "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nn =  hist.history['accuracy'][-1]\n",
        "precision_nn = precision_score(y_test, y_pred_nn_classes, average='weighted')\n",
        "recall_nn = recall_score(y_test, y_pred_nn_classes, average='weighted')\n",
        "f1_nn = f1_score(y_test, y_pred_nn_classes, average='weighted')\n",
        "confusion_nn = confusion_matrix(y_test, y_pred_nn_classes)\n",
        "\n",
        "# Store the metrics in the same dictionary\n",
        "metrics_dict['accuracy']['NN'] = accuracy_nn\n",
        "metrics_dict['precision']['NN'] = precision_nn\n",
        "metrics_dict['recall']['NN'] = recall_nn\n",
        "metrics_dict['f1_score']['NN'] = f1_nn\n",
        "metrics_dict['confusion_matrix']['NN'] = confusion_nn\n",
        "\n",
        "print(f\"Accuracy (NN): {accuracy_nn}\")\n",
        "print(f\"Precision (NN): {precision_nn}\")\n",
        "print(f\"Recall (NN): {recall_nn}\")\n",
        "print(f\"F1-score (NN): {f1_nn}\")\n",
        "print(f\"Confusion Matrix (NN):\\n{confusion_nn}\\n\")\n",
        "\n",
        "# Interpretation of model predictions for NN\n",
        "print(\"\\nExample predictions for NN:\")\n",
        "for i in range(min(5, len(y_test))):   # Print 5 samples or less\n",
        "    pred_value = y_pred_nn_classes[i]\n",
        "    actual_value = y_test.iloc[i]\n",
        "    print(f\"Sample {i+1}: Predicted={pred_value}, Actual={actual_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loTUChJO2eTc"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYsKa5szPSPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5962439e-2aef-402b-bea9-bc53e9b64158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5527333581256972\n",
            "Precision: 0.5255857355729238\n",
            "Recall: 0.5527333581256972\n",
            "F1-score: 0.5331319399180312\n",
            "Confusion Matrix:\n",
            "[[  68  230  213   40  212   67  250  190  241  185  135  100  147]\n",
            " [   0 1368    0    0  227    0  430    0   10    0    0    0    0]\n",
            " [  29    0 1240    0    0  215    0  193    0  202    0   21  190]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [  35  258    4    0 1064  185  135    1  265    0    5   38   98]\n",
            " [ 170  124   85   13  375  599  166   47  298   39   17   24   91]\n",
            " [ 100  348    3    0   44    2 1263    1  225    0    0    0    0]\n",
            " [  57  208  198   31   71    8  147 1182    2  117    0    8   43]\n",
            " [  43  112    1    0  159  165  366    5 1134    4   42   15   46]\n",
            " [   4    0    2    0    6    1    1   49  104 1858    0    1    2]\n",
            " [  34    0   99    5   15   30    9   21   11    2 1145  360  366]\n",
            " [  44    5  196    7   65  108   36   46   50    9  179 1257  118]\n",
            " [  59   43  167   11  108   10   75  151   54   26  432  335  580]]\n",
            "\n",
            "\n",
            "Example predictions for Logistic Regression:\n",
            "Sample 1: Predicted=6, Actual=6\n",
            "Sample 2: Predicted=1, Actual=1\n",
            "Sample 3: Predicted=8, Actual=8\n",
            "Sample 4: Predicted=1, Actual=1\n",
            "Sample 5: Predicted=4, Actual=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Create and train the Logistic Regression model\n",
        "logistic_regression = LogisticRegression()\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Store the accuracy\n",
        "metrics_dict['accuracy']['Logistic Regression'] = accuracy\n",
        "metrics_dict['precision']['Logistic Regression'] = precision\n",
        "metrics_dict['recall']['Logistic Regression'] = recall\n",
        "metrics_dict['f1_score']['Logistic Regression'] = f1\n",
        "metrics_dict['confusion_matrix']['Logistic Regression'] = confusion\n",
        "\n",
        "\n",
        "# Print evaluation metrics and confusion matrix\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion}\\n\")\n",
        "\n",
        "# Interpretation of model predictions for Logistic Regression\n",
        "print(\"\\nExample predictions for Logistic Regression:\")\n",
        "for i in range(min(5, len(y_test))):   # Print 5 samples or less\n",
        "    pred_value = y_pred[i]\n",
        "    actual_value = y_test.iloc[i]\n",
        "    print(f\"Sample {i+1}: Predicted={pred_value}, Actual={actual_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis"
      ],
      "metadata": {
        "id": "QFUxHxiAgQtr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22CUaCgV8F5B"
      },
      "source": [
        "Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52l7idPiJI0F"
      },
      "outputs": [],
      "source": [
        "# # Print evaluation metrics\n",
        "# for metric, values in metrics_dict.items():\n",
        "#     print(f\"\\n{metric.capitalize()}:\\n\")\n",
        "#     for clf_name, value in values.items():\n",
        "#         print(f\"{clf_name}: {value}\")\n",
        "\n",
        "# # Identify the best-performing model based on accuracy\n",
        "# best_accuracy_model = max(metrics_dict['accuracy'], key=metrics_dict['accuracy'].get)\n",
        "# print(f\"\\nBest Model based on Accuracy: {best_accuracy_model} with Accuracy {metrics_dict['accuracy'][best_accuracy_model]:.4f}\")\n",
        "\n",
        "# # Identify the best-performing model based on F1-score\n",
        "# best_f1_model = max(metrics_dict['f1_score'], key=metrics_dict['f1_score'].get)\n",
        "# print(f\"\\nBest Model based on F1-score: {best_f1_model} with F1-score {metrics_dict['f1_score'][best_f1_model]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXdGizVWTazc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb08ff9-a649-45b6-9eb8-8b5a0cea48b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: KNN\n",
            "Accuracy: 0.9403\n",
            "Precision: 0.9411\n",
            "Recall: 0.9403\n",
            "F1-score: 0.9361\n",
            "Confusion Matrix:\n",
            "[[1139   57   44   23  173   97   91  121  109  101   42   24   57]\n",
            " [   0 2035    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0 2090    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [   4    0    0    0 2079    0    2    0    2    0    0    0    1]\n",
            " [  23    1    1    0   61 1911   11    2   32    1    2    0    3]\n",
            " [   2    3    0    0    0    0 1965   12    4    0    0    0    0]\n",
            " [   3    0    0    0    0    0    7 2056    5    1    0    0    0]\n",
            " [   3    0    0    0    2    0    8    9 2070    0    0    0    0]\n",
            " [   2    0    0    0    0    0    0    0    3 2022    0    0    1]\n",
            " [   3    0    0    0    6    1    0    3    1    0 1994   74   15]\n",
            " [   2    0    0    0   14    0    0    0    0    1   95 1988   20]\n",
            " [  30    0    0    0   24    7    3    4    6    1   91   53 1832]]\n",
            "\n",
            "Error: 'accuracy' key not found for model: Linear Regression\n",
            "Model: SVM\n",
            "Accuracy: 0.9534\n",
            "Precision: 0.9515\n",
            "Recall: 0.9534\n",
            "F1-score: 0.9514\n",
            "Confusion Matrix:\n",
            "[[1367   62   42   26  109   79   73   61   93   65   24   19   58]\n",
            " [   1 2034    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0 2090    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [  25    0    0    0 2056    3    1    0    3    0    0    0    0]\n",
            " [  86    0    0    0   25 1928    3    0    4    0    0    0    2]\n",
            " [   8    4    0    0    0    0 1963   11    0    0    0    0    0]\n",
            " [   3    0    0    0    0    0   17 2046    5    1    0    0    0]\n",
            " [  12    0    0    0    2    0    5    8 2063    1    0    0    1]\n",
            " [   7    0    0    0    0    0    0    0    0 2021    0    0    0]\n",
            " [  29    0    0    0    0    0    0    0    0    0 1997   58   13]\n",
            " [  26    0    0    0    1    0    0    0    0    0   53 2020   20]\n",
            " [  51    0    0    0    3    2    0    0    2    0   23   22 1948]]\n",
            "\n",
            "Model: NN\n",
            "Accuracy: 0.9654\n",
            "Precision: 0.9499\n",
            "Recall: 0.9513\n",
            "F1-score: 0.9494\n",
            "Confusion Matrix:\n",
            "[[1377   49   41   31  117   60   83   62   71   57   35   22   73]\n",
            " [   2 2029    0    0    0    0    4    0    0    0    0    0    0]\n",
            " [   0    0 2090    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [  15    0    0    0 2064    3    2    0    0    0    2    0    2]\n",
            " [ 102    0    0    0   37 1889    4    0   11    0    1    0    4]\n",
            " [   5    2    0    0    0    1 1977    0    1    0    0    0    0]\n",
            " [   9    0    0    0    0    0   29 2028    5    0    0    0    1]\n",
            " [  19    0    0    0    4    1    8    4 2054    0    2    0    0]\n",
            " [  16    0    0    0    0    0    0    1    0 2009    1    0    1]\n",
            " [   7    0    0    0    0    0    0    0    0    0 2037   34   19]\n",
            " [  12    0    0    0    2    2    0    0    0    0  116 1968   20]\n",
            " [  40    0    0    0    3    5    1    0    0    0   31   17 1954]]\n",
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.5527\n",
            "Precision: 0.5256\n",
            "Recall: 0.5527\n",
            "F1-score: 0.5331\n",
            "Confusion Matrix:\n",
            "[[  68  230  213   40  212   67  250  190  241  185  135  100  147]\n",
            " [   0 1368    0    0  227    0  430    0   10    0    0    0    0]\n",
            " [  29    0 1240    0    0  215    0  193    0  202    0   21  190]\n",
            " [   0    0    0 2105    0    0    0    0    0    0    0    0    0]\n",
            " [  35  258    4    0 1064  185  135    1  265    0    5   38   98]\n",
            " [ 170  124   85   13  375  599  166   47  298   39   17   24   91]\n",
            " [ 100  348    3    0   44    2 1263    1  225    0    0    0    0]\n",
            " [  57  208  198   31   71    8  147 1182    2  117    0    8   43]\n",
            " [  43  112    1    0  159  165  366    5 1134    4   42   15   46]\n",
            " [   4    0    2    0    6    1    1   49  104 1858    0    1    2]\n",
            " [  34    0   99    5   15   30    9   21   11    2 1145  360  366]\n",
            " [  44    5  196    7   65  108   36   46   50    9  179 1257  118]\n",
            " [  59   43  167   11  108   10   75  151   54   26  432  335  580]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def analyze_results(metrics_dict):\n",
        "    # Initialize variables to track best-performing model\n",
        "    best_model = None\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    # Iterate through each model in the metrics dictionary\n",
        "    for model_name, model_metrics in metrics_dict['accuracy'].items():\n",
        "        # Check if 'accuracy' key is present in model_metrics\n",
        "        if model_metrics is None:\n",
        "            print(f\"Error: 'accuracy' key not found for model: {model_name}\")\n",
        "            continue\n",
        "\n",
        "        accuracy = model_metrics\n",
        "        precision = metrics_dict['precision'].get(model_name)\n",
        "        recall = metrics_dict['recall'].get(model_name)\n",
        "        f1 = metrics_dict['f1_score'].get(model_name)\n",
        "        confusion_matrix = metrics_dict['confusion_matrix'].get(model_name)\n",
        "\n",
        "        # Interpretation of model predictions\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\" if precision is not None else \"Precision: N/A\")\n",
        "        print(f\"Recall: {recall:.4f}\" if recall is not None else \"Recall: N/A\")\n",
        "        print(f\"F1-score: {f1:.4f}\" if f1 is not None else \"F1-score: N/A\")\n",
        "        print(f\"Confusion Matrix:\\n{confusion_matrix}\\n\" if confusion_matrix is not None else \"Confusion Matrix: N/A\\n\")\n",
        "\n",
        "        # Update best-performing model if necessary\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model_name\n",
        "\n",
        "    return best_model, best_accuracy\n",
        "\n",
        "# Assuming you have metrics_dict defined somewhere\n",
        "\n",
        "# Call analyze_results to get the best model and accuracy\n",
        "best_accuracy_model, best_accuracy = analyze_results(metrics_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identification of the best-performing model\n",
        "if best_accuracy_model is not None:\n",
        "    print(f\"Best-performing model: {best_accuracy_model} (Accuracy: {best_accuracy:.4f})\")\n",
        "else:\n",
        "    print(\"No models with accuracy information found.\")"
      ],
      "metadata": {
        "id": "FQpVek3_fT3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee66c87a-10f0-4b70-b5bd-6c6abe9833c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best-performing model: NN (Accuracy: 0.9654)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strengths and weaknesses"
      ],
      "metadata": {
        "id": "CTwufWVNdrBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiiGBduuTjVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca4cb23-3ea9-40d9-aeae-c6ee250002dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN: Strengths: Simple and easy to implement, but sensitive to outliers and requires careful selection of k.\n",
            "Linear Regression: Strengths: Interpretable model suitable for linear relationships, but sensitive to outliers and assumes linearity.\n",
            "Neural Networks: Strengths: Powerful and flexible for complex patterns, but requires large datasets, prone to overfitting, and lacks interpretability.\n",
            "SVM: Strengths: Effective in high-dimensional spaces with different kernels, but memory-intensive and requires careful hyperparameter tuning.\n"
          ]
        }
      ],
      "source": [
        "strengths_weaknesses = {\n",
        "    'kNN': 'Strengths: Simple and easy to implement, but sensitive to outliers and requires careful selection of k.',\n",
        "    'Linear Regression': 'Strengths: Interpretable model suitable for linear relationships, but sensitive to outliers and assumes linearity.',\n",
        "    'Neural Networks': 'Strengths: Powerful and flexible for complex patterns, but requires large datasets, prone to overfitting, and lacks interpretability.',\n",
        "    'SVM': 'Strengths: Effective in high-dimensional spaces with different kernels, but memory-intensive and requires careful hyperparameter tuning.'\n",
        "}\n",
        "\n",
        "for model, description in strengths_weaknesses.items():\n",
        "        print(f\"{model}: {description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights into factors contributing to performance variation"
      ],
      "metadata": {
        "id": "v6OJYUHKdx0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nInsights into factors contributing to performance variation:\")\n",
        "print(\"- k-Nearest Neighbors (kNN):\")\n",
        "print(\"  - Feature Engineering: Check if features are well-preprocessed.\")\n",
        "print(\"  - Hyperparameters: Tune k value for optimal performance.\")\n",
        "print(\"  - Data Quality: Address outliers and missing values.\")\n",
        "print(\"  - Model Architecture: Simple and effective for many cases.\")\n",
        "print(\"  - Sample Size: Larger datasets can improve accuracy.\")\n",
        "\n",
        "print(\"\\n- Support Vector Machines (SVM):\")\n",
        "print(\"  - Feature Engineering: Optimize feature selection and scaling.\")\n",
        "print(\"  - Hyperparameters: Choose appropriate kernel and regularization parameters.\")\n",
        "print(\"  - Data Quality: Handle outliers and ensure clean data.\")\n",
        "print(\"  - Model Architecture: Effective for complex decision boundaries.\")\n",
        "print(\"  - Sample Size: Requires sufficient data for robust performance.\")\n",
        "\n",
        "print(\"\\n- Neural Networks (NN):\")\n",
        "print(\"  - Feature Engineering: Data normalization and feature scaling are crucial.\")\n",
        "print(\"  - Hyperparameters: Tune learning rate, batch size, and network architecture.\")\n",
        "print(\"  - Data Quality: Clean and preprocess data to reduce noise.\")\n",
        "print(\"  - Model Architecture: Design deep architectures based on problem complexity.\")\n",
        "print(\"  - Sample Size: Deep learning often benefits from large datasets.\")\n",
        "\n",
        "print(\"\\n- Logistic Regression:\")\n",
        "print(\"  - Feature Engineering: Encode categorical variables and handle missing values.\")\n",
        "print(\"  - Hyperparameters: Adjust regularization strength for model complexity.\")\n",
        "print(\"  - Data Quality: Ensure data quality and appropriate preprocessing.\")\n",
        "print(\"  - Model Architecture: Linear model suitable for binary classification tasks.\")\n",
        "print(\"  - Sample Size: Requires adequate samples per class for reliable predictions.\")\n",
        "\n",
        "print(\"\\n- Linear Regression:\")\n",
        "print(\"  - Feature Engineering: Include relevant features and handle multicollinearity.\")\n",
        "print(\"  - Hyperparameters: Regularization can prevent overfitting.\")\n",
        "print(\"  - Data Quality: Clean data and handle outliers.\")\n",
        "print(\"  - Model Architecture: Linear relationship assumption between features and target.\")\n",
        "print(\"  - Sample Size: Requires enough data points for reliable regression results.\")\n"
      ],
      "metadata": {
        "id": "HMvYOx7taGR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3e6421-3530-4ff4-f6ba-76d27da0633e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Insights into factors contributing to performance variation:\n",
            "- k-Nearest Neighbors (kNN):\n",
            "  - Feature Engineering: Check if features are well-preprocessed.\n",
            "  - Hyperparameters: Tune k value for optimal performance.\n",
            "  - Data Quality: Address outliers and missing values.\n",
            "  - Model Architecture: Simple and effective for many cases.\n",
            "  - Sample Size: Larger datasets can improve accuracy.\n",
            "\n",
            "- Support Vector Machines (SVM):\n",
            "  - Feature Engineering: Optimize feature selection and scaling.\n",
            "  - Hyperparameters: Choose appropriate kernel and regularization parameters.\n",
            "  - Data Quality: Handle outliers and ensure clean data.\n",
            "  - Model Architecture: Effective for complex decision boundaries.\n",
            "  - Sample Size: Requires sufficient data for robust performance.\n",
            "\n",
            "- Neural Networks (NN):\n",
            "  - Feature Engineering: Data normalization and feature scaling are crucial.\n",
            "  - Hyperparameters: Tune learning rate, batch size, and network architecture.\n",
            "  - Data Quality: Clean and preprocess data to reduce noise.\n",
            "  - Model Architecture: Design deep architectures based on problem complexity.\n",
            "  - Sample Size: Deep learning often benefits from large datasets.\n",
            "\n",
            "- Logistic Regression:\n",
            "  - Feature Engineering: Encode categorical variables and handle missing values.\n",
            "  - Hyperparameters: Adjust regularization strength for model complexity.\n",
            "  - Data Quality: Ensure data quality and appropriate preprocessing.\n",
            "  - Model Architecture: Linear model suitable for binary classification tasks.\n",
            "  - Sample Size: Requires adequate samples per class for reliable predictions.\n",
            "\n",
            "- Linear Regression:\n",
            "  - Feature Engineering: Include relevant features and handle multicollinearity.\n",
            "  - Hyperparameters: Regularization can prevent overfitting.\n",
            "  - Data Quality: Clean data and handle outliers.\n",
            "  - Model Architecture: Linear relationship assumption between features and target.\n",
            "  - Sample Size: Requires enough data points for reliable regression results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion"
      ],
      "metadata": {
        "id": "KTMiKx7ee2-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap of key findings and conclusions\n"
      ],
      "metadata": {
        "id": "bYpAZhjzd6P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRecap of Key Findings and Conclusions:\")\n",
        "\n",
        "# Strengths and Weaknesses Recap\n",
        "print(\"\\nStrengths and Weaknesses Recap:\")\n",
        "print(\"- k-Nearest Neighbors (kNN): Simple and easy to implement but sensitive to outliers.\")\n",
        "print(\"- Support Vector Machines (SVM): Effective in high-dimensional spaces but memory-intensive.\")\n",
        "print(\"- Neural Networks (NN): Powerful for complex patterns but requires large datasets.\")\n",
        "print(\"- Logistic Regression: Interpretable for binary classification but assumes linear relationship.\")\n",
        "print(\"- Linear Regression: Interpretable for linear relationships but sensitive to outliers.\")\n",
        "\n",
        "# Insights into Factors Recap\n",
        "print(\"\\nInsights into Factors Recap:\")\n",
        "print(\"- Feature Engineering: Critical for all models to preprocess data and handle outliers.\")\n",
        "print(\"- Hyperparameters: Importance of tuning parameters for optimal model performance.\")\n",
        "print(\"- Data Quality: Clean and quality data is fundamental for accurate predictions.\")\n",
        "print(\"- Model Architecture: Consideration of model complexity and suitability for the problem.\")\n",
        "print(\"- Sample Size: Larger datasets often lead to better model performance.\")\n",
        "print(\"- Domain-Specific Insights: Understanding the problem context aids in model selection.\")\n",
        "\n",
        "# Overall Conclusion\n",
        "print(\"\\nOverall Conclusion:\")\n",
        "print(\"Based on the comparative analysis, the choice of model depends on various factors including dataset size, complexity of the problem, interpretability requirements, and computational resources.\")\n"
      ],
      "metadata": {
        "id": "yshoQTgMaG0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29158349-ce69-4298-c32e-7a4b042ccd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recap of Key Findings and Conclusions:\n",
            "\n",
            "Strengths and Weaknesses Recap:\n",
            "- k-Nearest Neighbors (kNN): Simple and easy to implement but sensitive to outliers.\n",
            "- Support Vector Machines (SVM): Effective in high-dimensional spaces but memory-intensive.\n",
            "- Neural Networks (NN): Powerful for complex patterns but requires large datasets.\n",
            "- Logistic Regression: Interpretable for binary classification but assumes linear relationship.\n",
            "- Linear Regression: Interpretable for linear relationships but sensitive to outliers.\n",
            "\n",
            "Insights into Factors Recap:\n",
            "- Feature Engineering: Critical for all models to preprocess data and handle outliers.\n",
            "- Hyperparameters: Importance of tuning parameters for optimal model performance.\n",
            "- Data Quality: Clean and quality data is fundamental for accurate predictions.\n",
            "- Model Architecture: Consideration of model complexity and suitability for the problem.\n",
            "- Sample Size: Larger datasets often lead to better model performance.\n",
            "- Domain-Specific Insights: Understanding the problem context aids in model selection.\n",
            "\n",
            "Overall Conclusion:\n",
            "Based on the comparative analysis, the choice of model depends on various factors including dataset size, complexity of the problem, interpretability requirements, and computational resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the best model for activity recognition based on the MHEALTH\n",
        "dataset."
      ],
      "metadata": {
        "id": "_w3JKuOJeDBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out None values (Linear Resgression)\n",
        "filtered_accuracy = {k: v for k, v in metrics_dict['accuracy'].items() if v is not None}\n",
        "\n",
        "# Find the best-performing model\n",
        "if filtered_accuracy:\n",
        "    best_accuracy_model = max(filtered_accuracy, key=filtered_accuracy.get)\n",
        "    best_accuracy = filtered_accuracy[best_accuracy_model]\n",
        "    precision = metrics_dict['precision'][best_accuracy_model]\n",
        "    recall = metrics_dict['recall'][best_accuracy_model]\n",
        "    f1_score = metrics_dict['f1_score'][best_accuracy_model]\n",
        "\n",
        "    # Summary of the best model based on metrics\n",
        "    print(\"\\nSummary of the Best Model for Activity Recognition (MHEALTH Dataset):\")\n",
        "    print(f\"- The best-performing model based on accuracy was found to be {best_accuracy_model} with an accuracy of {best_accuracy:.4f}.\")\n",
        "    print(f\"- {best_accuracy_model} showed high precision ({precision:.4f}), recall ({recall:.4f}), and F1-score ({f1_score:.4f}), indicating overall good performance.\")\n",
        "    print(\"- Key factors contributing to the model's success include its ability to learn complex patterns, adapt to non-linear relationships, and handle high-dimensional data.\")\n",
        "    print(\"- Feature engineering, hyperparameter tuning, and data preprocessing played crucial roles in optimizing the model's performance.\")\n",
        "    print(f\"- {best_accuracy_model}'s interpretability may be a concern, but its predictive power and performance metrics outweigh this limitation for activity recognition in the MHEALTH dataset.\")\n",
        "else:\n",
        "    print(\"No valid accuracy values found in the metrics dictionary.\")\n"
      ],
      "metadata": {
        "id": "BV3jyAUwcNTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee037ef-91fb-48ef-ca74-9dbb3442ec6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of the Best Model for Activity Recognition (MHEALTH Dataset):\n",
            "- The best-performing model based on accuracy was found to be NN with an accuracy of 0.9654.\n",
            "- NN showed high precision (0.9499), recall (0.9513), and F1-score (0.9494), indicating overall good performance.\n",
            "- Key factors contributing to the model's success include its ability to learn complex patterns, adapt to non-linear relationships, and handle high-dimensional data.\n",
            "- Feature engineering, hyperparameter tuning, and data preprocessing played crucial roles in optimizing the model's performance.\n",
            "- NN's interpretability may be a concern, but its predictive power and performance metrics outweigh this limitation for activity recognition in the MHEALTH dataset.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yyF69R6F5aIR",
        "7SImCp9R5hrP",
        "-vqTipRTyJxt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}